---
title: "P8105 Homework 5"
author: "Siyue Gao"
date: '`r format(Sys.time(), "%Y-%m-%d")`'
output: github_document
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
	echo = TRUE,
	message = FALSE,
	warning = FALSE,
	fig.height = 6,
	out.width = "90%",
	dpi = 200
)
```

```{r}
library(tidyverse)
```

```{r, include = FALSE}
theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```


## Problem 1

Start with a dataframe containing all file names.

```{r}
file_names = list.files(path = "data/problem1", full.names = TRUE)
```

Iterate over file names and read in data for each subject using `purrr::map`. The results are saved as `files`.

```{r}
files = map(file_names, read_csv)

for (i in 1:20) {
  
  if (i < 11) {
    
    files[[i]] = files[[i]] %>% 
      mutate(
        subject_id = i,
        arm = "Control"
      )
    
  }
  
  else {
    
    files[[i]] = files[[i]] %>% 
      mutate(
        subject_id = i - 10,
        arm = "Experimental"
      )

  }
  
}
```

Tidy the result dataframe by `bind_rows()` and `pivot_longer()`.

```{r}
result_df = files %>% 
  bind_rows() %>% 
  select(subject_id, everything()) %>% 
  pivot_longer(
    week_1:week_8,
    names_to = "week",
    names_prefix = "week_",
    values_to = "observation"
  ) %>% 
  mutate(
    subject_id = as.character(subject_id),
    week = as.numeric(week)
  )
```

Make a spaghetti plot showing observations on each subject over time, and comment on differences between groups.

```{r}
result_df %>% 
  ggplot(aes(x = week, y = observation, type = subject_id, color = arm)) +
  geom_line() +
  labs(
    x = "Week",
    y = "Observation Values",
    color = "Arm",
    title = "Observations on each subject over time by study arm"
  )
```

On average, the observation values for those in experimental arm are higher than those in control arm. In addition, there seems to be some increasing over time in experimental arm, while the observation value tends to be more constant for those in control arm.


## Problem 2

### Data Description

Import the data.

```{r}
homicide = read_csv("data/homicide-data.csv")
```

The `homicide` dataset contains `r nrow(homicide)` cases, with `r ncol(homicide)` variables, mainly describing the homicide cases in `r n_distinct(homicide$city)` cities in U.S. These variables include reported date and location, some information on victim (name, race, age, and sex), and the final disposition of the homicide.

### Summary on Homicides

Create a `city_state` and a `result` variable. The `result` variable is used as a binary indicator of "solved" of "unsolved" status of the homicide.

With further inspection, there is an observation from Tulsa, with a state recorded as "AL". Tulsa is located in Oklahoma, and this observation might be an entry error. Thus, I re-coded it as "Tulsa, OK".

```{r}
homicide = homicide %>% 
  mutate(
    state = str_to_upper(state),
    city_state = str_c(city, ", ", state),
    city_state = recode(city_state, "Tulsa, AL" = "Tulsa, OK"),
    result = case_when(
      disposition == "Closed by arrest"      ~ "solved",
      disposition == "Closed without arrest" ~ "unsolved",
      disposition == "Open/No arrest"        ~ "unsolved",
      TRUE                                   ~ ""
    )
  )
```

Produce a summary table of total number of homicides and the number of unsolved homicides within each city.

```{r}
homicide %>% 
  group_by(city_state) %>% 
  summarise(
    total_number = n(),
    unsolved_number = sum(result == "unsolved")
  ) %>% 
  knitr::kable(
    col.names = c("City, State", "Total Number of Homicides", "Number of Unsolved Homicides")
  )
```

### For Baltimore, MD

```{r}
summary = homicide %>% 
  filter(city_state == "Baltimore, MD") %>% 
  summarise(
    total_number = n(),
    unsolved_number = sum(result == "unsolved")
  )

baltimore_prop_test = 
  prop.test(
    summary %>% pull(unsolved_number), 
    summary %>% pull(total_number)
    )

broom::tidy(baltimore_prop_test)
```

The estimated proportion of unsolved homicides for the city of Baltimore, MD, is **`r round(broom::tidy(baltimore_prop_test) %>% pull(estimate), digits = 3)`**, with a 95% confidence interval of **(`r round(broom::tidy(baltimore_prop_test) %>% pull(conf.low), digits = 3)`, `r round(broom::tidy(baltimore_prop_test) %>% pull(conf.high), digits = 3)`)**.

### Iterate on Each City

Write a function `prop_test`.

```{r}
prop_test = function(df) {
  
  summary = 
    df %>% 
    summarise(
      total_number = n(),
      unsolved_number = sum(result == "unsolved")
      ) 
  
  city_prop_test = 
    prop.test(
      summary %>% pull(unsolved_number), 
      summary %>% pull(total_number)
      ) %>% 
    broom::tidy()
  
  city_prop_test
  
}
```

To iterate for each city, first, we'd like to nest the irrelevant columns. Then, apply the function to the nested dataframe and only keep those variables in need as `final_result` dataframe.

```{r}
homicide_nest_df = homicide %>% 
  select(city_state, everything()) %>% 
  nest(city_homicide = uid:result)

homicide_nest_df =
  homicide_nest_df %>% 
  mutate(
    city_prop_test = map(city_homicide, prop_test)
  ) %>% 
  unnest(city_prop_test) 

final_result = 
  homicide_nest_df %>%
  select(city_state, estimate, conf.low, conf.high)
```

To give some illustrative examples of the `final_result` on `prop.test` for each single city, print the first 6 lines of this dataframe.

```{r}
final_result %>% 
  head(6) %>% 
  knitr::kable(
    digits = 3,
    col.names = c("City, State", "Estimated Proportion", "Lower 95% CI", "Upper 95% CI")
  )
```

### Forest Plot

Create a plot that shows the estimates and CIs for each city.

```{r}
estimate_ci_plot = final_result %>% 
  mutate(
    city_state = fct_reorder(city_state, estimate)
  ) %>% 
  ggplot(aes(x = city_state, y = estimate)) +
  geom_point() +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) +
  labs(
    x = "City, State",
    y = "Estimated Proportion of Unsolved Homicides",
    title = "Estimated Proportion of Unsolved Homicides with 95% Confidence Interval in Each City"
  ) +
  theme(
    axis.text.x = element_text(angle = 90),
    axis.title = element_text(face = "bold")
  )

ggsave(
  estimate_ci_plot,
  filename = "results/estimate_plot.png",
  width = 20,
  height = 16,
  units = "cm",
  bg = "white"
)

knitr::include_graphics("results/estimate_plot.png")
```

Among these 50 large U.S. cities included in the dataset, Richmond, VA, has the lowest estimated proportion of unsolved homicides, while Chicago, IL, has the highest.


## Problem 3

Write a function to pull the estimate and p-value.

*   Fix n = 30
*   Fix $\sigma$ = 5

```{r}
set.seed(1)

sim_power = function(n = 30, mu, sigma = 5) {
  
  x = rnorm(n = n, mean = mu, sd = sigma)
  
  output = 
    t.test(x, mu = 0) %>% 
    broom::tidy()
  
  tibble(
    estimate_hat = output %>% pull(estimate),
    p_value = output %>% pull(p.value)
  )
  
}
```

### Set $\mu$ = 0

Use list columns to achieve it. Save the $\hat{\mu}$ and p-value in `sim_results_df`.

```{r, cache = TRUE}
sim_results_df = 
  expand_grid(
    true_mean = 0,
    iteration = 1:5000
  ) %>% 
  mutate(
    power_df = map(.x = true_mean, ~ sim_power(mu = .x))
  ) %>% 
  unnest(power_df)
```

### Repeat for $\mu$ = {1, 2, 3, 4, 5, 6}

```{r, cache = TRUE}
re_sim_results_df = 
  expand_grid(
    true_mean = c(1:6),
    iteration = 1:5000
  ) %>% 
  mutate(
    power_df = map(.x = true_mean, ~ sim_power(mu = .x))
  ) %>% 
  unnest(power_df)
```

*   Make a plot showing the proportion of times the null was rejected on the y axis and the true value of $\mu$ on the x axis.

```{r}
re_sim_results_df %>% 
  mutate(
    decision = case_when(
      p_value < 0.05  ~ "reject",
      p_value >= 0.05 ~ "fail to reject",
      TRUE            ~ ""
    )
  ) %>% 
  group_by(true_mean) %>% 
  summarise(
    n_obs = n(),
    prop = sum(decision == "reject") / n_obs * 100
  ) %>% 
  ggplot(aes(x = true_mean, y = prop)) +
  geom_point() +
  geom_path(alpha = .5) +
  labs(
    x = expression(mu),
    y = "Proportion of Times Rejecting Null\n(%)",
    title = expression(paste("Proportion of Times the Null was Rejected vs. True Value of ", mu)),
    caption = expression(paste("Significant level: ", alpha, " = 0.05"))
  ) + 
  scale_x_continuous(
    breaks = 1:6
  )
```

As we can observe from the above scatterplot, the proportion of times the null was rejected increases as  the true mean becomes larger, indicating that the detectable effect size and power are positively correlated.

*   Make a plot showing the average estimate of $\hat{\mu}$ on the y axis and the true value of $\mu$ on the x axis.

```{r}
re_sim_results_df %>% 
  group_by(true_mean) %>% 
  summarise(
    mean_estimate = mean(estimate_hat)
  ) %>% 
  ggplot(aes(x = true_mean, y = mean_estimate)) +
  geom_point() +
  #geom_path(alpha = .5) +
  labs(
    x = expression(mu),
    y = "Average Estimate",
    title = expression(paste("Average Estimate of ", hat(mu), " vs. True Value of ", mu)),
    caption = expression(paste("Significant level: ", alpha, " = 0.05"))
  ) + 
  scale_x_continuous(
    breaks = 1:6
  )
```

